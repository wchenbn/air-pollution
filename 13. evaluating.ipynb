{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/zafir/miniconda3/envs/tensorflow/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Lambda, Reshape, Dropout\n",
    "from tensorflow.keras.layers import Bidirectional, RepeatVector, Dot, Activation\n",
    "from tensorflow.keras.layers import Concatenate, Flatten\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoder_input_data = np.load('./data/third-order/Centar/train_encoder_input_data.npy')\n",
    "train_decoder_input_data = np.load('./data/third-order/Centar/train_decoder_input_data.npy')\n",
    "train_decoder_target_data = np.load('./data/third-order/Centar/train_decoder_target_data.npy')\n",
    "\n",
    "valid_encoder_input_data = np.load('./data/third-order/Centar/valid_encoder_input_data.npy')\n",
    "valid_decoder_input_data = np.load('./data/third-order/Centar/valid_decoder_input_data.npy')\n",
    "valid_decoder_target_data = np.load('./data/third-order/Centar/valid_decoder_target_data.npy')\n",
    "\n",
    "test_encoder_input_data = np.load('./data/third-order/Centar/test_encoder_input_data.npy')\n",
    "test_decoder_input_data = np.load('./data/third-order/Centar/test_decoder_input_data.npy')\n",
    "test_decoder_target_data = np.load('./data/third-order/Centar/test_decoder_target_data.npy')\n",
    "test_decoder_target_data = test_decoder_target_data.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx, encoder_input_dim = (train_encoder_input_data.shape[1], \n",
    "                         train_encoder_input_data.shape[2])\n",
    "    \n",
    "Ty, decoder_input_dim = (train_decoder_input_data.shape[1], \n",
    "                         train_decoder_input_data.shape[2])\n",
    "\n",
    "decoder_output_dim = train_decoder_target_data.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 150\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, using the best hyperparameters found during the random search, we build a model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function standard_lstm at 0x7f9874d0f488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x7f9874d0f488>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_lstm at 0x7f9874d0f488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x7f9874d0f488>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_lstm at 0x7f9874d0f510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x7f9874d0f510>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_lstm at 0x7f9874d0f510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x7f9874d0f510>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function standard_lstm at 0x7f9874d0f488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x7f9874d0f488>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_lstm at 0x7f9874d0f488> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x7f9874d0f488>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_lstm at 0x7f9874d0f510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x7f9874d0f510>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_lstm at 0x7f9874d0f510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x7f9874d0f510>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# ------------------- SHARED LAYERS ---------------------\n",
    "encoder_lstm = LSTM(64, return_state=True, \n",
    "                      name='encoder_lstm')\n",
    "decoder_lstm = LSTM(64, return_state=True, \n",
    "                    return_sequences=True, name='decoder_lstm')\n",
    "decoder_dense = Dense(decoder_output_dim, \n",
    "                      activation='linear', name='decoder_dense')\n",
    "\n",
    "# Since the best standard model was not stacked and the dense dropout\n",
    "# rate was 0, we basically can remove the Dropout layers.\n",
    "\n",
    "# -------------------- TRAIN MODEL ----------------------\n",
    "encoder_inputs = Input(shape=(Tx, encoder_input_dim), \n",
    "                       name='encoder_inputs')\n",
    "\n",
    "# Obtain the hidden states of the encoder\n",
    "_, h, c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "decoder_inputs = Input(shape=(Ty, decoder_input_dim), \n",
    "                       name='decoder_inputs')\n",
    "\n",
    "# Obtain the outputs of the decoder (we don't care about\n",
    "# the hidden states during training)\n",
    "x, _, _ = decoder_lstm(decoder_inputs, initial_state=[h, c])\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], \n",
    "              outputs=decoder_outputs)\n",
    "optimizer = Adam(learning_rate=0.000219)\n",
    "model.compile(optimizer=optimizer, loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train the model and save the best version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf \"./logs/standard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60677 samples, validate on 3388 samples\n",
      "   64/60677 [..............................] - ETA: 27:42 - loss: 1.0787WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.211889). Check your callbacks.\n",
      "60677/60677 [==============================] - 68s 1ms/sample - loss: 0.1497 - val_loss: 0.0528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f986ac56748>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[train_encoder_input_data, \n",
    "            train_decoder_input_data], \n",
    "          y=train_decoder_target_data,\n",
    "          validation_data=([\n",
    "            valid_encoder_input_data,\n",
    "            valid_decoder_input_data],\n",
    "            valid_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', \n",
    "                                  patience=patience, \n",
    "                                  verbose=1),\n",
    "                     ModelCheckpoint('./checkpoints/standard.h5', \n",
    "                                     save_best_only=True),\n",
    "                     TensorBoard(log_dir=\"./logs/standard\", \n",
    "                                 histogram_freq=1)]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the best (trained) model, obtain the layers and build an inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model('./checkpoints/standard.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_lstm = best_model.get_layer('encoder_lstm')\n",
    "decoder_lstm = best_model.get_layer('decoder_lstm')\n",
    "decoder_dense = best_model.get_layer('decoder_dense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# ------------------ INFERENCE MODEL --------------------\n",
    "encoder_inputs = Input(shape=(Tx, encoder_input_dim), \n",
    "                       name='encoder_inputs')\n",
    "\n",
    "# Obtain the hidden states of the encoder\n",
    "_, h, c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "decoder_inputs = Input(shape=(Ty, decoder_input_dim), \n",
    "                       name='decoder_inputs')\n",
    "\n",
    "x = Lambda(lambda z: z[:, 0, :])(decoder_inputs)\n",
    "x = K.expand_dims(x, axis=1)\n",
    "\n",
    "outputs = []\n",
    "for t in range(Ty):\n",
    "    # Obtain the output and hidden states of the decoder LSTM \n",
    "    out, h, c = decoder_lstm(x, initial_state=[h, c])\n",
    "    out = decoder_dense(out)\n",
    "    out = Flatten()(out)\n",
    "    outputs.append(out)\n",
    "\n",
    "    # Prepare the input for the next timestep by removing the \n",
    "    # ground truth value for PM in the previous step and \n",
    "    # concatenating the calculated output value. Do this only\n",
    "    # in case there is a next timestep to be processed.\n",
    "    if t < Ty - 1:\n",
    "        x = Lambda(lambda z: z[:, t+1, 1:])(decoder_inputs)\n",
    "        x = Concatenate(axis=-1)([out, xx])\n",
    "        x = K.expand_dims(x, axis=1)\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], \n",
    "              outputs=outputs)\n",
    "optimizer = Adam(learning_rate=0.000219)\n",
    "model.compile(optimizer=optimizer, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_model_output(y_pred):\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = np.swapaxes(y_pred, 0, 1)\n",
    "    y_pred = y_pred.flatten()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict([test_encoder_input_data, test_decoder_input_data])\n",
    "y_pred = reformat_model_output(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48393303"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = K.eval(tf.keras.losses.mean_squared_error(test_decoder_target_data, y_pred))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
