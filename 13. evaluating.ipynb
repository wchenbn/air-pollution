{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/zafir/miniconda3/envs/tensorflow/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Lambda, Reshape, Dropout\n",
    "from tensorflow.keras.layers import Bidirectional, RepeatVector, Dot, Activation\n",
    "from tensorflow.keras.layers import Concatenate, Flatten\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoder_input_data = np.load('./data/third-order/Centar/train_encoder_input_data.npy')\n",
    "train_decoder_input_data = np.load('./data/third-order/Centar/train_decoder_input_data.npy')\n",
    "train_decoder_target_data = np.load('./data/third-order/Centar/train_decoder_target_data.npy')\n",
    "\n",
    "valid_encoder_input_data = np.load('./data/third-order/Centar/valid_encoder_input_data.npy')\n",
    "valid_decoder_input_data = np.load('./data/third-order/Centar/valid_decoder_input_data.npy')\n",
    "valid_decoder_target_data = np.load('./data/third-order/Centar/valid_decoder_target_data.npy')\n",
    "\n",
    "test_encoder_input_data = np.load('./data/third-order/Centar/test_encoder_input_data.npy')\n",
    "test_decoder_input_data = np.load('./data/third-order/Centar/test_decoder_input_data.npy')\n",
    "test_decoder_target_data = np.load('./data/third-order/Centar/test_decoder_target_data.npy')\n",
    "test_decoder_target_data = test_decoder_target_data.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train, Tx, encoder_input_dim = train_encoder_input_data.shape\n",
    "    \n",
    "Ty, decoder_input_dim = (train_decoder_input_data.shape[1], \n",
    "                         train_decoder_input_data.shape[2])\n",
    "\n",
    "decoder_output_dim = train_decoder_target_data.shape[2]\n",
    "\n",
    "m_val = valid_encoder_input_data.shape[0]\n",
    "m_test = test_decoder_input_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 150\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, using the best hyperparameters found during the random search, we fine tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64\n",
    "dense_dropout_rate = 0.1\n",
    "learning_rate = 0.000219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# ------------------- SHARED LAYERS ---------------------\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, \n",
    "                      name='encoder_lstm')\n",
    "decoder_lstm = LSTM(latent_dim, return_state=True, \n",
    "                    return_sequences=True, name='decoder_lstm')\n",
    "decoder_dense = Dense(decoder_output_dim, \n",
    "                      activation='linear', name='decoder_dense')\n",
    "dense_dropout = Dropout(rate=dense_dropout_rate, name='dense_dropout')\n",
    "\n",
    "# Since the best standard model was not stacked and the dense dropout\n",
    "# rate was 0, we basically can remove the Dropout layers.\n",
    "\n",
    "# -------------------- TRAIN MODEL ----------------------\n",
    "encoder_inputs = Input(shape=(Tx, encoder_input_dim), \n",
    "                       name='encoder_inputs')\n",
    "\n",
    "# Obtain the hidden states of the encoder\n",
    "_, h, c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "decoder_inputs = Input(shape=(Ty, decoder_input_dim), \n",
    "                       name='decoder_inputs')\n",
    "\n",
    "# Obtain the outputs of the decoder (we don't care about\n",
    "# the hidden states during training)\n",
    "x, _, _ = decoder_lstm(decoder_inputs, initial_state=[h, c])\n",
    "x = dense_dropout(x)\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], \n",
    "              outputs=decoder_outputs)\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf \"./logs/standard\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60677 samples, validate on 3388 samples\n",
      "60677/60677 [==============================] - 40s 665us/sample - loss: 0.1596 - val_loss: 0.0575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f80f41c67b8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[train_encoder_input_data, \n",
    "            train_decoder_input_data], \n",
    "          y=train_decoder_target_data,\n",
    "          validation_data=([\n",
    "            valid_encoder_input_data,\n",
    "            valid_decoder_input_data],\n",
    "            valid_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=1,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', \n",
    "                                  patience=patience, \n",
    "                                  verbose=1),\n",
    "                     ModelCheckpoint('./checkpoints/standard.h5', \n",
    "                                     save_best_only=True),\n",
    "                     TensorBoard(log_dir=\"./logs/standard\", \n",
    "                                 histogram_freq=1)]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the best (trained) model, obtain the layers and build an inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model('./checkpoints/standard.h5')\n",
    "\n",
    "encoder_lstm = best_model.get_layer('encoder_lstm')\n",
    "decoder_lstm = best_model.get_layer('decoder_lstm')\n",
    "decoder_dense = best_model.get_layer('decoder_dense')\n",
    "dense_dropout = best_model.get_layer('dense_dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "encoder_inputs = Input(shape=(Tx, encoder_input_dim), \n",
    "                       name='encoder_inputs')\n",
    "\n",
    "decoder_inputs = Input(shape=(Ty, decoder_input_dim), \n",
    "                       name='decoder_inputs')\n",
    "\n",
    "# Obtain the hidden states of the encoder\n",
    "_, h, c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "outputs = []\n",
    "for t in range(Ty):\n",
    "    if t == 0:\n",
    "        x = Lambda(lambda z: z[:, t, :])(decoder_inputs)\n",
    "    else:\n",
    "        x = Lambda(lambda z: z[:, t, 1:])(decoder_inputs)\n",
    "        x = Concatenate(axis=-1)([out, x])\n",
    "    \n",
    "    x = K.expand_dims(x, axis=1)\n",
    "    \n",
    "    # Obtain the output and hidden states of the decoder LSTM \n",
    "    out, h, c = decoder_lstm(x, initial_state=[h, c])\n",
    "    out = dense_dropout(out)\n",
    "    out = decoder_dense(out)\n",
    "    out = Flatten()(out)\n",
    "    outputs.append(out)\n",
    "\n",
    "inference_model = Model(inputs=[encoder_inputs, decoder_inputs], \n",
    "              outputs=outputs, name='inference_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6625962"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = inference_model.predict([test_encoder_input_data, test_decoder_input_data])\n",
    "y_pred = format_model_output(y_pred)\n",
    "loss = K.eval(tf.keras.losses.mean_squared_error(test_decoder_target_data, y_pred))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attentive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(encoder_outputs, h_prev, attention_repeat, \n",
    "                       attention_concatenate, attention_dense_1,\n",
    "                       attention_dense_2, attention_activation,\n",
    "                       attention_dot):\n",
    "    \n",
    "    x = attention_repeat(h_prev)\n",
    "    x = attention_concatenate([encoder_outputs, x])\n",
    "    x = attention_dense_1(x)\n",
    "    energies = attention_dense_2(x)\n",
    "    alphas = attention_activation(energies)\n",
    "    context = attention_dot([alphas, encoder_outputs])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_attentive_model(encoder_latent_dim, decoder_latent_dim,\n",
    "                           attention_dense_dim, seq_dropout_rate,\n",
    "                           dense_dropout_rate, learning_rate):\n",
    "    K.clear_session()\n",
    "\n",
    "    # ------------------- SHARED LAYERS ---------------------\n",
    "    # Encoder layers\n",
    "    encoder_lstm = Bidirectional(LSTM(encoder_latent_dim, return_sequences=True, \n",
    "                                      name='encoder_lstm'), merge_mode='concat')\n",
    "\n",
    "    # Attention layers\n",
    "    attention_repeat = RepeatVector(n=Tx, name='attention_repeat')\n",
    "    attention_concatenate = Concatenate(axis=-1, name='attention_concatenate')\n",
    "    attention_dense_1 = Dense(attention_dense_dim, activation='tanh', \n",
    "                              name='attention_dense_1')\n",
    "    attention_dense_2 = Dense(1, activation='relu', name='attention_dense_2')\n",
    "    attention_activation = Activation(softmax, name='attention_activation') \n",
    "    attention_dot = Dot(axes=1, name='attention_dot')\n",
    "\n",
    "    # Decoder layers\n",
    "    decoder_concatenate = Concatenate(axis=-1, name='decoder_concatenate')\n",
    "    decoder_lstm = LSTM(decoder_latent_dim, return_state=True, \n",
    "                        name='decoder_lstm')\n",
    "    decoder_dense = Dense(decoder_output_dim, activation='linear',\n",
    "                          name='decoder_dense')\n",
    "\n",
    "    seq_dropout = Dropout(rate=seq_dropout_rate, name='seq_dropout')\n",
    "    dense_dropout = Dropout(rate=dense_dropout_rate, name='dense_dropout')\n",
    "\n",
    "    # -------------------- TRAIN MODEL ----------------------\n",
    "    encoder_inputs = Input(shape=(Tx, encoder_input_dim), \n",
    "                           name='encoder_inputs')\n",
    "\n",
    "    x = encoder_lstm(encoder_inputs)\n",
    "    encoder_outputs = seq_dropout(x)\n",
    "\n",
    "    decoder_inputs = Input(shape=(Ty, decoder_input_dim), \n",
    "                           name='decoder_inputs')\n",
    "    h0 = Input(shape=(decoder_latent_dim,), name='h0')\n",
    "    c0 = Input(shape=(decoder_latent_dim,), name='c0')\n",
    "    h, c = h0, c0\n",
    "\n",
    "    # Decoder outputs\n",
    "    outputs = []\n",
    "\n",
    "    for t in range(Ty):\n",
    "        context = one_step_attention(encoder_outputs, h, attention_repeat, \n",
    "                                     attention_concatenate, attention_dense_1,\n",
    "                                     attention_dense_2, attention_activation,\n",
    "                                     attention_dot)\n",
    "\n",
    "        # Obtain the decoder input at timestamp t\n",
    "        x = Lambda(lambda z: z[:, t, :])(decoder_inputs)\n",
    "        decoder_input = Reshape((1, x.shape[1]))(x)\n",
    "\n",
    "        # Construct the full decoder input by concatenating the input at \n",
    "        # timestemp t with the calculated context\n",
    "        full_decoder_input = decoder_concatenate([decoder_input, context])\n",
    "\n",
    "        h, _, c = decoder_lstm(full_decoder_input, initial_state=[h, c])\n",
    "        x = dense_dropout(h)\n",
    "        decoder_output = decoder_dense(x)\n",
    "\n",
    "        outputs.append(decoder_output)\n",
    "\n",
    "    model = Model(inputs=[encoder_inputs, decoder_inputs, h0, c0], \n",
    "                  outputs=outputs)\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, using the best hyperparameters found during the random search, we fine tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_latent_dim = 64\n",
    "decoder_latent_dim = 64\n",
    "attention_dense_dim = 12\n",
    "seq_dropout_rate = 0.2\n",
    "dense_dropout_rate = 0.2\n",
    "learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0_train = np.zeros((m_train, decoder_latent_dim))\n",
    "c0_train = np.zeros((m_train, decoder_latent_dim))\n",
    "\n",
    "h0_val = np.zeros((m_val, decoder_latent_dim))\n",
    "c0_val = np.zeros((m_val, decoder_latent_dim))\n",
    "\n",
    "h0_test = np.zeros((m_test, decoder_latent_dim))\n",
    "c0_test = np.zeros((m_test, decoder_latent_dim))\n",
    "\n",
    "# due to the model architecture, we need to transform the output shape and type\n",
    "train_attentive_decoder_target_data = list(np.swapaxes(\n",
    "                                              train_decoder_target_data, 0, 1))\n",
    "valid_attentive_decoder_target_data = list(np.swapaxes(\n",
    "                                              valid_decoder_target_data, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_attentive_model(encoder_latent_dim, decoder_latent_dim,\n",
    "                               attention_dense_dim, seq_dropout_rate,\n",
    "                               dense_dropout_rate, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf \"./logs/attentive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-26 14:01:55]\tepoch: 0\tloss: 1.96127\tval_loss:1.21645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fefa489f470>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[train_encoder_input_data, \n",
    "            train_decoder_input_data,\n",
    "            h0_train, c0_train], \n",
    "          y=train_attentive_decoder_target_data,\n",
    "          validation_data=([\n",
    "            valid_encoder_input_data,\n",
    "            valid_decoder_input_data, \n",
    "            h0_val, c0_val],\n",
    "            valid_attentive_decoder_target_data),\n",
    "          batch_size=batch_size,\n",
    "          epochs=1,\n",
    "          verbose=0,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', \n",
    "                                  patience=patience, \n",
    "                                  verbose=1),\n",
    "                     LossPrintingCallback(Ty),\n",
    "                     ModelCheckpoint('./checkpoints/attentive',\n",
    "                                     save_weights_only=True,\n",
    "                                     save_best_only=True),\n",
    "                     TensorBoard(log_dir=\"./logs/attentive\", \n",
    "                                 histogram_freq=1)]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, load the best (trained) model, obtain the layers and build an inference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = create_attentive_model(encoder_latent_dim, decoder_latent_dim,\n",
    "                               attention_dense_dim, seq_dropout_rate,\n",
    "                               dense_dropout_rate, learning_rate)\n",
    "\n",
    "best_model.load_weights('./checkpoints/attentive')\n",
    "\n",
    "# Encoder layers\n",
    "encoder_lstm = best_model.get_layer('bidirectional')\n",
    "\n",
    "# Attention layers\n",
    "attention_repeat = best_model.get_layer('attention_repeat')\n",
    "attention_concatenate = best_model.get_layer('attention_concatenate')\n",
    "attention_dense_1 = best_model.get_layer('attention_dense_1')\n",
    "attention_dense_2 = best_model.get_layer('attention_dense_2')\n",
    "attention_activation = best_model.get_layer('attention_activation')\n",
    "attention_dot = best_model.get_layer('attention_dot')\n",
    "\n",
    "# Decoder layers\n",
    "decoder_concatenate = best_model.get_layer('decoder_concatenate')\n",
    "decoder_lstm = best_model.get_layer('decoder_lstm')\n",
    "decoder_dense = best_model.get_layer('decoder_dense')\n",
    "\n",
    "seq_dropout = best_model.get_layer('seq_dropout')\n",
    "dense_dropout = best_model.get_layer('dense_dropout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Inference model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
